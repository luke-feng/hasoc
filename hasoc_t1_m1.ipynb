{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\r\n",
    "\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import json\r\n",
    "import tqdm\r\n",
    "from transformers import BertTokenizer, BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import logging\r\n",
    "from transformers import BartTokenizer, BartModel\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from pytorch_lightning import Trainer\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "\r\n",
    "# Huggingface transformers\r\n",
    "import transformers\r\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn ,cuda\r\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "import pytorch_lightning as pl\r\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
    "\r\n",
    "#handling html data\r\n",
    "import seaborn as sns\r\n",
    "from pylab import rcParams\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib import rc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\r\n",
    "                    level = logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = 'D:/UZH/other/hasoc/new/rawdata/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "raw_datasets = pd.read_csv(path+'t1_without_ge.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\r\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def tokenize_function(examples):\r\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "mlb = LabelBinarizer()\r\n",
    "y = raw_datasets['task_1'].tolist()\r\n",
    "yt = mlb.fit_transform(y)\r\n",
    "yt = torch.LongTensor(yt)\r\n",
    "\r\n",
    "x = raw_datasets['text'].tolist()\r\n",
    "\r\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, yt , test_size=0.1, random_state=42,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\r\n",
    "class HasocmDataset (Dataset):\r\n",
    "    def __init__(self,quest,tags, tokenizer, max_len):\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.text = quest\r\n",
    "        self.labels = tags\r\n",
    "        self.max_len = max_len\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.text)\r\n",
    "    \r\n",
    "    def __getitem__(self, item_idx):\r\n",
    "        text = self.text[item_idx]\r\n",
    "        inputs = self.tokenizer.encode_plus(\r\n",
    "            text,\r\n",
    "            None,\r\n",
    "            add_special_tokens=True, # Add [CLS] [SEP]\r\n",
    "            max_length= self.max_len,\r\n",
    "            padding = 'max_length',\r\n",
    "            return_token_type_ids= False,\r\n",
    "            return_attention_mask= True, # Differentiates padded vs normal token\r\n",
    "            truncation=True, # Truncate data beyond max length\r\n",
    "            return_tensors = 'pt' # PyTorch Tensor format\r\n",
    "          )\r\n",
    "        \r\n",
    "        input_ids = inputs['input_ids'].flatten()\r\n",
    "        attn_mask = inputs['attention_mask'].flatten()\r\n",
    "        #token_type_ids = inputs[\"token_type_ids\"]\r\n",
    "        \r\n",
    "        return {\r\n",
    "            'input_ids': input_ids ,\r\n",
    "            'attention_mask': attn_mask,\r\n",
    "            'label': torch.tensor(self.labels[item_idx], dtype=torch.float)\r\n",
    "            \r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class HasocDataModule (pl.LightningDataModule):    \r\n",
    "    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,batch_size=16,max_token_len=200):\r\n",
    "        super().__init__()\r\n",
    "        self.tr_text = x_tr\r\n",
    "        self.tr_label = y_tr\r\n",
    "        self.val_text = x_val\r\n",
    "        self.val_label = y_val\r\n",
    "        self.test_text = x_test\r\n",
    "        self.test_label = y_test\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.max_token_len = max_token_len\r\n",
    "\r\n",
    "    def setup(self):\r\n",
    "        self.train_dataset = HasocmDataset(quest=self.tr_text,  tags=self.tr_label,tokenizer=self.tokenizer,max_len= self.max_token_len)\r\n",
    "        self.val_dataset= HasocmDataset(quest=self.val_text, tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\r\n",
    "        self.test_dataset =HasocmDataset(quest=self.test_text, tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\r\n",
    "        \r\n",
    "        \r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = True , num_workers=4)\r\n",
    "\r\n",
    "    def val_dataloader(self):\r\n",
    "        return DataLoader (self.val_dataset,batch_size= self.batch_size , num_workers=4)\r\n",
    "\r\n",
    "    def test_dataloader(self):\r\n",
    "        return DataLoader (self.test_dataset,batch_size= self.batch_size , num_workers=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\r\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\r\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\r\n",
    "N_EPOCHS = 20\r\n",
    "BATCH_SIZE = 32\r\n",
    "MAX_LEN = 150\r\n",
    "LR = 1e-04\r\n",
    "Bert_tokenizer = tokenizer\r\n",
    "datamodule = HasocDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,Bert_tokenizer,BATCH_SIZE,MAX_LEN)\r\n",
    "datamodule.setup()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class HasocClassifier(pl.LightningModule):\r\n",
    "    # Set up the classifier\r\n",
    "    def __init__(self,steps_per_epoch=None,n_epochs=3, lr=2e-5):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.bert=BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n",
    "        # self.bert = BartModel.from_pretrained('facebook/bart-large', return_dict=True)\r\n",
    "        # self.bilstm = nn.LSTM(self.bert.config.hidden_size, 256, bidirectional=True)\r\n",
    "        self.layer1 = nn.Linear(self.bert.config.hidden_size, 128)\r\n",
    "        self.layer2 = nn.Linear(128, 32)\r\n",
    "        self.layer3 = nn.Linear(32, 1)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "        self.dropout = nn.Dropout(p=0.1)\r\n",
    "        self.steps_per_epoch = steps_per_epoch\r\n",
    "        self.n_epochs = n_epochs\r\n",
    "        self.lr = lr\r\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\r\n",
    "\r\n",
    "    def forward(self,input_ids, attn_mask):\r\n",
    "        output = self.bert(input_ids=input_ids,attention_mask=attn_mask)   \r\n",
    "        output = self.layer1(output.pooler_output)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer2(output)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer3(output)\r\n",
    "        output = self.sigmoid(output)\r\n",
    "        return output\r\n",
    "\r\n",
    "    def training_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('train_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\r\n",
    "\r\n",
    "\r\n",
    "    def validation_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('val_loss',loss , prog_bar=True,logger=True)        \r\n",
    "        return loss\r\n",
    "\r\n",
    "    def test_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('test_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return loss\r\n",
    "    \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = AdamW(self.parameters() , lr=self.lr)\r\n",
    "        warmup_steps = self.steps_per_epoch//3\r\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\r\n",
    "\r\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\r\n",
    "\r\n",
    "        return [optimizer], [scheduler]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\r\n",
    "model = HasocClassifier( steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)\r\n",
    "\r\n",
    "checkpoint_callback = ModelCheckpoint(\r\n",
    "    monitor='val_loss',# monitored quantity\r\n",
    "    filename='QTag-{epoch:02d}-{val_loss:.2f}',\r\n",
    "    save_top_k=3, #  save the top 3 models\r\n",
    "    mode='min', # mode of the monitored quantity  for optimization\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback],progress_bar_refresh_rate = 30)\r\n",
    "trainer.fit(model, datamodule)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "c:\\users\\luke-\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages\\pytorch_lightning\\core\\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | bert      | BertModel         | 177 M \n",
      "1 | layer1    | Linear            | 98.4 K\n",
      "2 | layer2    | Linear            | 4.1 K \n",
      "3 | layer3    | Linear            | 33    \n",
      "4 | relu      | ReLU              | 0     \n",
      "5 | sigmoid   | Sigmoid           | 0     \n",
      "6 | dropout   | Dropout           | 0     \n",
      "7 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.824   Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = BertModel.from_pretrained(\"hasoc\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at hasoc were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "last_hidden_states = outputs.pooler_output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "last_hidden_states = outputs.pooler_output"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "411726c55619be18c0bdfac3340a3948968825b669dc59d8dab255e9205e9336"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
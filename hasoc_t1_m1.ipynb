{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\r\n",
    "\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import json\r\n",
    "import tqdm\r\n",
    "from transformers import BertTokenizer, BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import logging\r\n",
    "from transformers import BartTokenizer, BartModel\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from pytorch_lightning import Trainer\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "\r\n",
    "# Huggingface transformers\r\n",
    "import transformers\r\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn ,cuda\r\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "import pytorch_lightning as pl\r\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
    "\r\n",
    "#handling html data\r\n",
    "import seaborn as sns\r\n",
    "from pylab import rcParams\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib import rc\r\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\r\n",
    "                    level = logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "raw_datasets = pd.read_csv('t1_without_ge.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def tokenize_function(examples):\r\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "mlb = LabelBinarizer()\r\n",
    "y = raw_datasets['task_1'].tolist()\r\n",
    "yt = mlb.fit_transform(y)\r\n",
    "yt = torch.FloatTensor(yt)\r\n",
    "\r\n",
    "x = raw_datasets['text'].tolist()\r\n",
    "\r\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, yt , test_size=0.1, random_state=42,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\r\n",
    "class HasocmDataset (Dataset):\r\n",
    "    def __init__(self,quest,tags, tokenizer, max_len):\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.text = quest\r\n",
    "        self.labels = tags\r\n",
    "        self.max_len = max_len\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.text)\r\n",
    "    \r\n",
    "    def __getitem__(self, item_idx):\r\n",
    "        text = self.text[item_idx]\r\n",
    "        inputs = self.tokenizer.encode_plus(\r\n",
    "            text,\r\n",
    "            None,\r\n",
    "            add_special_tokens=True, # Add [CLS] [SEP]\r\n",
    "            max_length= self.max_len,\r\n",
    "            padding = 'max_length',\r\n",
    "            return_token_type_ids= False,\r\n",
    "            return_attention_mask= True, # Differentiates padded vs normal token\r\n",
    "            truncation=True, # Truncate data beyond max length\r\n",
    "            return_tensors = 'pt' # PyTorch Tensor format\r\n",
    "          )\r\n",
    "        \r\n",
    "        input_ids = inputs['input_ids'].flatten()\r\n",
    "        attn_mask = inputs['attention_mask'].flatten()\r\n",
    "        #token_type_ids = inputs[\"token_type_ids\"]\r\n",
    "        \r\n",
    "        return {\r\n",
    "            'input_ids': input_ids ,\r\n",
    "            'attention_mask': attn_mask,\r\n",
    "            'label': self.labels[item_idx]\r\n",
    "            \r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class HasocDataModule (pl.LightningDataModule):    \r\n",
    "    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,batch_size=16,max_token_len=200):\r\n",
    "        super().__init__()\r\n",
    "        self.tr_text = x_tr\r\n",
    "        self.tr_label = y_tr\r\n",
    "        self.val_text = x_val\r\n",
    "        self.val_label = y_val\r\n",
    "        self.test_text = x_test\r\n",
    "        self.test_label = y_test\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.max_token_len = max_token_len\r\n",
    "\r\n",
    "    def setup(self):\r\n",
    "        self.train_dataset = HasocmDataset(quest=self.tr_text,  tags=self.tr_label,tokenizer=self.tokenizer,max_len= self.max_token_len)\r\n",
    "        self.val_dataset= HasocmDataset(quest=self.val_text, tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\r\n",
    "        self.test_dataset =HasocmDataset(quest=self.test_text, tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\r\n",
    "        \r\n",
    "        \r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = True , num_workers=4)\r\n",
    "\r\n",
    "    def val_dataloader(self):\r\n",
    "        return DataLoader (self.val_dataset,batch_size= self.batch_size , num_workers=4)\r\n",
    "\r\n",
    "    def test_dataloader(self):\r\n",
    "        return DataLoader (self.test_dataset,batch_size= self.batch_size , num_workers=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "Tokenizer_NAME = \"bert-base-multilingual-cased\"\r\n",
    "tokenizer = BertTokenizer.from_pretrained(Tokenizer_NAME)\r\n",
    "BERT_MODEL_NAME = 'bert-base-multilingual-cased'\r\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\r\n",
    "N_EPOCHS = 20\r\n",
    "BATCH_SIZE = 16\r\n",
    "MAX_LEN = 150\r\n",
    "LR = 1e-04\r\n",
    "Bert_tokenizer = tokenizer\r\n",
    "datamodule = HasocDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,Bert_tokenizer,BATCH_SIZE,MAX_LEN)\r\n",
    "datamodule.setup()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class HasocClassifier(pl.LightningModule):\r\n",
    "    # Set up the classifier\r\n",
    "    def __init__(self,steps_per_epoch=None,n_epochs=3, lr=2e-5):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.bert=BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\r\n",
    "        # self.bert = BartModel.from_pretrained('facebook/bart-large', return_dict=True)\r\n",
    "        # self.bilstm = nn.LSTM(self.bert.config.hidden_size, 256, bidirectional=True)\r\n",
    "        self.layer1 = nn.Linear(self.bert.config.hidden_size, 128)\r\n",
    "        self.layer2 = nn.Linear(128, 32)\r\n",
    "        self.layer3 = nn.Linear(32, 1)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "        self.dropout = nn.Dropout(p=0.1)\r\n",
    "        self.steps_per_epoch = steps_per_epoch\r\n",
    "        self.n_epochs = n_epochs\r\n",
    "        self.lr = lr\r\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\r\n",
    "\r\n",
    "    def forward(self,input_ids, attn_mask):\r\n",
    "        output = self.bert(input_ids=input_ids,attention_mask=attn_mask)   \r\n",
    "        output = self.layer1(output.pooler_output)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer2(output)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer3(output)\r\n",
    "        output = self.sigmoid(output)\r\n",
    "        return output\r\n",
    "\r\n",
    "    def training_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('train_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\r\n",
    "\r\n",
    "\r\n",
    "    def validation_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('val_loss',loss , prog_bar=True,logger=True)        \r\n",
    "        return loss\r\n",
    "\r\n",
    "    def test_step(self,batch,batch_idx):\r\n",
    "        input_ids = batch['input_ids']\r\n",
    "        attention_mask = batch['attention_mask']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(input_ids,attention_mask)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('test_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return loss\r\n",
    "    \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = AdamW(self.parameters() , lr=self.lr)\r\n",
    "        warmup_steps = self.steps_per_epoch//3\r\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\r\n",
    "\r\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\r\n",
    "\r\n",
    "        return [optimizer], [scheduler]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\r\n",
    "model = HasocClassifier( steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)\r\n",
    "\r\n",
    "checkpoint_callback = ModelCheckpoint(\r\n",
    "    monitor='val_loss',# monitored quantity\r\n",
    "    filename='QTag-{epoch:02d}-{val_loss:.2f}',\r\n",
    "    save_top_k=1, #  save the top 3 models\r\n",
    "    mode='min', # mode of the monitored quantity  for optimization\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback, EarlyStopping(monitor=\"val_loss\")],progress_bar_refresh_rate = 30, num_sanity_val_steps=0)\r\n",
    "trainer.fit(model, datamodule)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "c:\\users\\luke-\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages\\pytorch_lightning\\core\\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | bert      | BertModel         | 177 M \n",
      "1 | layer1    | Linear            | 98.4 K\n",
      "2 | layer2    | Linear            | 4.1 K \n",
      "3 | layer3    | Linear            | 33    \n",
      "4 | relu      | ReLU              | 0     \n",
      "5 | sigmoid   | Sigmoid           | 0     \n",
      "6 | dropout   | Dropout           | 0     \n",
      "7 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.824   Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:   0%|          | 0/1953 [00:00<?, ?it/s] "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "trainer.test(model,datamodule=datamodule)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 217/217 [00:08<00:00, 24.12it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.5487996935844421}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.5487996935844421}]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "len(y_test), len(x_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3471, 3471)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "from torch.utils.data import TensorDataset\r\n",
    "\r\n",
    "# Tokenize all questions in x_test\r\n",
    "input_ids = []\r\n",
    "attention_masks = []\r\n",
    "\r\n",
    "\r\n",
    "for quest in x_test:\r\n",
    "    encoded_quest =  Bert_tokenizer.encode_plus(\r\n",
    "                    quest,\r\n",
    "                    None,\r\n",
    "                    add_special_tokens=True,\r\n",
    "                    max_length= MAX_LEN,\r\n",
    "                    padding = 'max_length',\r\n",
    "                    return_token_type_ids= False,\r\n",
    "                    return_attention_mask= True,\r\n",
    "                    truncation=True,\r\n",
    "                    return_tensors = 'pt'      \r\n",
    "    )\r\n",
    "    \r\n",
    "    # Add the input_ids from encoded question to the list.    \r\n",
    "    input_ids.append(encoded_quest['input_ids'])\r\n",
    "    # Add its attention mask \r\n",
    "    attention_masks.append(encoded_quest['attention_mask'])\r\n",
    "    \r\n",
    "# Now convert the lists into tensors.\r\n",
    "input_ids = torch.cat(input_ids, dim=0)\r\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
    "labels = y_test\r\n",
    "\r\n",
    "# Set the batch size.  \r\n",
    "TEST_BATCH_SIZE = BATCH_SIZE  \r\n",
    "\r\n",
    "# Create the DataLoader.\r\n",
    "pred_data = TensorDataset(input_ids, attention_masks, labels)\r\n",
    "pred_sampler = SequentialSampler(pred_data)\r\n",
    "pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=TEST_BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "flat_pred_outs = 0\r\n",
    "flat_true_labels = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "load model from local checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model_path = ''\r\n",
    "# model = HasocClassifier.load_from_checkpoint(model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "model.to('cuda')\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HasocClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layer1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (layer3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "pred_outs, true_labels = [], []\r\n",
    "\r\n",
    "device = 'cuda'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "for batch in pred_dataloader:\r\n",
    "    # Add batch to GPU\r\n",
    "    batch = tuple(t.to(device) for t in batch)\r\n",
    "  \r\n",
    "    # Unpack the inputs from our dataloader\r\n",
    "    b_input_ids, b_attn_mask, b_labels = batch\r\n",
    " \r\n",
    "    with torch.no_grad():\r\n",
    "        # Forward pass, calculate logit predictions\r\n",
    "        pred_out = model(b_input_ids,b_attn_mask)\r\n",
    "        # pred_out = torch.sigmoid(pred_out)\r\n",
    "        # Move predicted output and labels to CPU\r\n",
    "        pred_out = pred_out.detach().cpu().numpy()\r\n",
    "        label_ids = b_labels.to('cpu').numpy()\r\n",
    "        #i+=1\r\n",
    "        # Store predictions and true labels\r\n",
    "        #print(i)\r\n",
    "        #print(outputs)\r\n",
    "        #print(logits)\r\n",
    "        #print(label_ids)\r\n",
    "    pred_outs.append(pred_out)\r\n",
    "    true_labels.append(label_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Combine the results across all batches. \r\n",
    "flat_pred_outs = np.concatenate(pred_outs,)\r\n",
    "\r\n",
    "# Combine the correct labels for each batch into a single list.\r\n",
    "flat_true_labels = np.concatenate(true_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "flat_pred_outs.shape , flat_true_labels.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3471, 1), (3471, 1))"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#define candidate threshold values\r\n",
    "threshold  = np.arange(0.4,0.61,0.01)\r\n",
    "threshold"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ,\n",
       "       0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ])"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "def classify(pred_prob,thresh):\r\n",
    "    y_pred = []\r\n",
    "\r\n",
    "    for tag_label_row in pred_prob:\r\n",
    "        temp=[]\r\n",
    "        for tag_label in tag_label_row:\r\n",
    "            if tag_label >= thresh:\r\n",
    "                temp.append(1) # Infer tag value as 1 (present)\r\n",
    "            else:\r\n",
    "                temp.append(0) # Infer tag value as 0 (absent)\r\n",
    "        y_pred.append(temp)\r\n",
    "\r\n",
    "    return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from sklearn import metrics\r\n",
    "scores=[] # Store the list of f1 scores for prediction on each threshold\r\n",
    "\r\n",
    "#convert labels to 1D array\r\n",
    "y_true = flat_true_labels.ravel() \r\n",
    "\r\n",
    "for thresh in threshold:\r\n",
    "    \r\n",
    "    #classes for each threshold\r\n",
    "    pred_bin_label = classify(flat_pred_outs,thresh) \r\n",
    "\r\n",
    "    #convert to 1D array\r\n",
    "    y_pred = np.array(pred_bin_label).ravel()\r\n",
    "\r\n",
    "    scores.append(metrics.f1_score(y_true,y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "opt_thresh = threshold[scores.index(max(scores))]\r\n",
    "print(f'Optimal Threshold Value = {opt_thresh}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal Threshold Value = 0.4700000000000001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "\r\n",
    "#predictions for optimal threshold\r\n",
    "y_pred_labels = classify(flat_pred_outs,opt_thresh)\r\n",
    "y_pred = np.array(y_pred_labels).ravel() # Flatten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "print(metrics.classification_report(y_true,y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.75      0.74      1286\n",
      "         1.0       0.85      0.84      0.85      2185\n",
      "\n",
      "    accuracy                           0.81      3471\n",
      "   macro avg       0.79      0.80      0.80      3471\n",
      "weighted avg       0.81      0.81      0.81      3471\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "y_pred = mlb.inverse_transform(np.array(y_pred_labels))\r\n",
    "y_act = mlb.inverse_transform(flat_true_labels)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "df = pd.DataFrame({'Body':x_test,'Actual Tags':y_act,'Predicted Tags':y_pred})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "df.sample(100)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   Body Actual Tags  \\\n",
       "3347  😂😜😂ऐसे कमीने दोस्त किसीको ना मिले हॉस्पिटल में...         NOT   \n",
       "267   @TheGArnab अगर धरना देने से सब सही हो जाता तो....         NOT   \n",
       "291   मैं भी एक मुसलमान हूं लेकिन मन्ने आजतक नहीं दे...         NOT   \n",
       "386   He has really took all four right backs and le...         HOF   \n",
       "610   अलविदा शेर-ए-बिहार…  अल्लाह आपकी मग़फ़िरत फरमा...         NOT   \n",
       "...                                                 ...         ...   \n",
       "1536  @indian_gokul @narendramodi This man completel...         NOT   \n",
       "1268  🤔🤔🤔🤔ob die och in meinem Trabi mitfährt🤔🤔🤔😂😂 h...         NOT   \n",
       "617   @TheRealOJ32 Get lost, butcher.      #Murderer...         HOF   \n",
       "747   RT @NarendraSaluja: पहले किसी ने बात नहीं की, ...         NOT   \n",
       "846   @RealHistoryPic अगर रडार वाला ट्वीट कहीं राहुल...         HOF   \n",
       "\n",
       "     Predicted Tags  \n",
       "3347            HOF  \n",
       "267             NOT  \n",
       "291             HOF  \n",
       "386             HOF  \n",
       "610             NOT  \n",
       "...             ...  \n",
       "1536            NOT  \n",
       "1268            NOT  \n",
       "617             HOF  \n",
       "747             NOT  \n",
       "846             HOF  \n",
       "\n",
       "[100 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Actual Tags</th>\n",
       "      <th>Predicted Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>😂😜😂ऐसे कमीने दोस्त किसीको ना मिले हॉस्पिटल में...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>@TheGArnab अगर धरना देने से सब सही हो जाता तो....</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>मैं भी एक मुसलमान हूं लेकिन मन्ने आजतक नहीं दे...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>He has really took all four right backs and le...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>अलविदा शेर-ए-बिहार…  अल्लाह आपकी मग़फ़िरत फरमा...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>@indian_gokul @narendramodi This man completel...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>🤔🤔🤔🤔ob die och in meinem Trabi mitfährt🤔🤔🤔😂😂 h...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>@TheRealOJ32 Get lost, butcher.      #Murderer...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>RT @NarendraSaluja: पहले किसी ने बात नहीं की, ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>@RealHistoryPic अगर रडार वाला ट्वीट कहीं राहुल...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_path = ''\r\n",
    "model = HasocClassifier.load_from_checkpoint(model_path)\r\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "411726c55619be18c0bdfac3340a3948968825b669dc59d8dab255e9205e9336"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
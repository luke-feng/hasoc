{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import logging\r\n",
    "\r\n",
    "from transformers import AutoTokenizer\r\n",
    "import json\r\n",
    "import tqdm\r\n",
    "from transformers import BertTokenizer, BertModel\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import logging\r\n",
    "from transformers import BartTokenizer, BartModel\r\n",
    "from sklearn.preprocessing import LabelBinarizer\r\n",
    "from pytorch_lightning import Trainer\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "\r\n",
    "# Huggingface transformers\r\n",
    "import transformers\r\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup, RobertaTokenizer,RobertaModel\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn ,cuda\r\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "import pytorch_lightning as pl\r\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
    "\r\n",
    "#handling html data\r\n",
    "import seaborn as sns\r\n",
    "from pylab import rcParams\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib import rc\r\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "raw_datasets = pd.read_csv('t1_without_ge.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\r\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "mlb = LabelBinarizer()\r\n",
    "y = raw_datasets['task_1'].tolist()\r\n",
    "yt = mlb.fit_transform(y)\r\n",
    "yt = torch.FloatTensor(yt)\r\n",
    "\r\n",
    "x = raw_datasets['text'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model_name = 'bert-base-multilingual-cased'\r\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\r\n",
    "model = BertModel.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_bertembedding(text):\r\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\r\n",
    "    embedding = model(**inputs).pooler_output\r\n",
    "    embedding = embedding.tolist()[0]\r\n",
    "    return embedding\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "par = tqdm.tqdm(total=len(x), ncols=100)\r\n",
    "emb = []\r\n",
    "for text in x:\r\n",
    "    embedding = get_bertembedding(text)\r\n",
    "    par.update(1)\r\n",
    "    emb.append(embedding)\r\n",
    "\r\n",
    "par.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|███████████████████████████████████████████████████████| 34705/34705 [1:09:46<00:00,  8.29it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df = pd.DataFrame({'text':x,'embedding':emb,'Tags':y})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df.to_csv('t1_emb_without_ge.tsv', sep='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(emb, yt , test_size=0.1, random_state=42,shuffle=True)\r\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42,shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class HasocmDataset (Dataset):\r\n",
    "    def __init__(self,embedding, labels):\r\n",
    "        self.embedding = embedding\r\n",
    "        self.labels = labels\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.embedding)\r\n",
    "    \r\n",
    "    def __getitem__(self, item_idx):\r\n",
    "        embedding = self.embedding[item_idx]\r\n",
    "        \r\n",
    "        return {\r\n",
    "            'embedding': embedding,\r\n",
    "            'label': self.labels[item_idx]\r\n",
    "            \r\n",
    "        }\r\n",
    "\r\n",
    "class HasocDataModule (pl.LightningDataModule):    \r\n",
    "    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,batch_size=16,max_token_len=200):\r\n",
    "        super().__init__()\r\n",
    "        self.tr_text = x_tr\r\n",
    "        self.tr_label = y_tr\r\n",
    "        self.val_text = x_val\r\n",
    "        self.val_label = y_val\r\n",
    "        self.test_text = x_test\r\n",
    "        self.test_label = y_test\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.max_token_len = max_token_len\r\n",
    "\r\n",
    "    def setup(self):\r\n",
    "        self.train_dataset = HasocmDataset(embedding=self.tr_text,  labels=self.tr_label)\r\n",
    "        self.val_dataset= HasocmDataset(embedding=self.val_text, labels=self.val_label)\r\n",
    "        self.test_dataset =HasocmDataset(embedding=self.test_text, labels=self.test_label)\r\n",
    "        \r\n",
    "        \r\n",
    "    def train_dataloader(self):\r\n",
    "        return DataLoader(self.train_dataset,batch_size= self.batch_size, shuffle = True , num_workers=4)\r\n",
    "\r\n",
    "    def val_dataloader(self):\r\n",
    "        return DataLoader (self.val_dataset,batch_size= self.batch_size , num_workers=4)\r\n",
    "\r\n",
    "    def test_dataloader(self):\r\n",
    "        return DataLoader (self.test_dataset,batch_size= self.batch_size , num_workers=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "N_EPOCHS = 20\r\n",
    "BATCH_SIZE = 512\r\n",
    "MAX_LEN = 150\r\n",
    "LR = 1e-04\r\n",
    "\r\n",
    "datamodule = HasocDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,BATCH_SIZE,MAX_LEN)\r\n",
    "datamodule.setup()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "class HasocClassifier(pl.LightningModule):\r\n",
    "    # Set up the classifier\r\n",
    "    def __init__(self,steps_per_epoch=None,n_epochs=3, lr=2e-5):\r\n",
    "        super().__init__()\r\n",
    "        self.layer1 = nn.Linear(768, 128)\r\n",
    "        self.layer2 = nn.Linear(128, 32)\r\n",
    "        self.layer3 = nn.Linear(32, 1)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "        self.dropout = nn.Dropout(p=0.1)\r\n",
    "        self.steps_per_epoch = steps_per_epoch\r\n",
    "        self.n_epochs = n_epochs\r\n",
    "        self.lr = lr\r\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\r\n",
    "\r\n",
    "    def forward(self,inputs):\r\n",
    "        output = self.layer1(inputs)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer2(output)\r\n",
    "        output = self.relu(output)\r\n",
    "        output = self.dropout(output)\r\n",
    "        output = self.layer3(output)\r\n",
    "        output = self.sigmoid(output)\r\n",
    "        return output\r\n",
    "\r\n",
    "    def training_step(self,batch,batch_idx):\r\n",
    "        embedding = batch['embedding']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(embedding)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('train_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\r\n",
    "\r\n",
    "\r\n",
    "    def validation_step(self,batch,batch_idx):\r\n",
    "        embedding = batch['embedding']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(embedding)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('val_loss',loss , prog_bar=True,logger=True)        \r\n",
    "        return loss\r\n",
    "\r\n",
    "    def test_step(self,batch,batch_idx):\r\n",
    "        embedding = batch['embedding']\r\n",
    "        labels = batch['label']\r\n",
    "        \r\n",
    "        outputs = self(embedding)\r\n",
    "        loss = self.criterion(outputs,labels)\r\n",
    "        self.log('test_loss',loss , prog_bar=True,logger=True)\r\n",
    "        \r\n",
    "        return loss\r\n",
    "    \r\n",
    "    \r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = AdamW(self.parameters() , lr=self.lr)\r\n",
    "        warmup_steps = self.steps_per_epoch//3\r\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\r\n",
    "\r\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\r\n",
    "\r\n",
    "        return [optimizer], [scheduler]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\r\n",
    "model = HasocClassifier( steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)\r\n",
    "\r\n",
    "checkpoint_callback = ModelCheckpoint(\r\n",
    "    monitor='val_loss',# monitored quantity\r\n",
    "    filename='QTag-{epoch:02d}-{val_loss:.2f}',\r\n",
    "    save_top_k=1, #  save the top 3 models\r\n",
    "    mode='min', # mode of the monitored quantity  for optimization\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(max_epochs = N_EPOCHS , gpus = 1, callbacks=[checkpoint_callback, EarlyStopping(monitor=\"val_loss\")],progress_bar_refresh_rate = 30, num_sanity_val_steps=0)\r\n",
    "trainer.fit(model, datamodule)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | layer1    | Linear            | 98.4 K\n",
      "1 | layer2    | Linear            | 4.1 K \n",
      "2 | layer3    | Linear            | 33    \n",
      "3 | relu      | ReLU              | 0     \n",
      "4 | sigmoid   | Sigmoid           | 0     \n",
      "5 | dropout   | Dropout           | 0     \n",
      "6 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "102 K     Trainable params\n",
      "0         Non-trainable params\n",
      "102 K     Total params\n",
      "0.410     Total estimated model params size (MB)\n",
      "c:\\users\\luke-\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:322: UserWarning: The number of training samples (49) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:   0%|          | 0/62 [00:00<00:00, 999.83it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = RobertaModel.from_pretrained('roberta-base')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "inputs1 = tokenizer(\"我是中国人\", return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "inputs2 = tokenizer(\"我是北京人\", return_tensors=\"pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "inputs1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 47876,  3602, 48569, 47643, 47516, 10809, 47973,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "inputs2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 47876,  3602, 48569, 48418,  6800, 46499, 11582, 47973,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "last_hidden_states = outputs.pooler_output "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "last_hidden_states.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "411726c55619be18c0bdfac3340a3948968825b669dc59d8dab255e9205e9336"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}